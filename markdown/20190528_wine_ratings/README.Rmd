---
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE
  , cache = TRUE
  , fig.align = "center"
  , fig.width = 8
  , fig.height = 5
  , dpi = 180
)
```

# Predicting Wine Ratings

In this notebook, we will go over how to import, explore, and model data to understand it.  All of the code & data is open source, but I hope you can take the themes or examples out of this and use it within the business.

![](./imgs/sponge-bob.jpg)

# Set Up

We are going to have to load in a few libraries with R's native command `library`.

## Packages for Part One

In part one of our analysis, the `tidyverse` will be used for our data import, data manipulation, and visualization.

```{r load-packages-1, warning=FALSE, message=FALSE  }
library(tidyverse) # ggplot, tibble, tidyr, readr, purrr, dplyr, stringr, forcats
library(skimr)     # quick data summaries
```


## Packages for Part Two

In part two of our analysis, the packages, `broom`, `Matrix`, `tidytext`, `glmnet`, `doParallel`, will be used for tidying model outputs, tying text data, and then prepping our data for penalized regression.

```{r load-packages-2, warning=FALSE, message=FALSE}
library(broom)     # tidy model outputs
library(tidytext)  # tidy text 
library(Matrix)    # for sparce matrix
library(glmnet)    # penalized regression
library(doParallel)# parallel processing
```

# Part one

## Data Pull

We are going to pull in a data set from a repository on `Github` using `readr`'s function `read_csv`. 

```{r pull-wine-ratings}
wine_ratings <- read_csv(
  "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-05-28/winemag-data-130k-v2.csv"
)
```

## Data Cleaning

+ See in the data imported correctly and/or see some of the top rows

```{r check-rows}
head(wine_ratings) # check top 5 observations
tail(wine_ratings) # check bottom 5 observations
```

Quick summary of data frame

+ Using `skim()` from the `skimr` package we get a nice overview of our dataframe.
  + We are probably going to have to address the `NA`s(**missing** values in `R`) in the categorical columns: `country`, `designation`, `province`, `region_1`, `region_2`,  `taster_name`, `taster_twitter_handle`, `variety`.
  + Additionally, we could do some type of imputation(fancy way of saying fill in or replace) the `NA` values in the `price` column. 
    + We also see that the our column `points` is normally distributed(good for a linear model), but our column `price` is heavily skewed(might want to transform it).

```{r summarize-df}
skim(wine_ratings) # quickly summarize our data frame
```


Data Cleanup
  
+ Model's don't deal with `NA`s or missing values very well, so:
  + For *categorical*, values we want to replace that `NA` with a character value of **missing** or **unknown**(the absence of information is information).
  + For *numeric* columns, there are loads of options like mean, median imputation or even some more advance techniques that I will talk about at some other time.
+ The `title` column also has some useful information that we will want to extract out of it for instance `year`.

Power of the **Pipe** a code-breakdown

+ The `%>%` or **Pipe** is the logical equivalent to a **then** statement... So, why is the unbelievably awesome?  As humans, we tend to work in a very procedural way, we take `thing` **then** do `other thing`... rinse and repeat
  + we want to create a new object `wine_df` and `<-` assign  `wine_ratings` **then** `%>%`
    + remove `X1` `select()` **then** `%>%`
    + replace our `NA`s `replace_na()` **then** `%>%`
    + pull year out of our `title` column `extract()` **then** `%>%`
    + change any weird `year` to values(3000 bc...) to `NA` with `mutate()` **then** `%>%`
    + remove any observations that don't have a price or points rating `filter()` **then** `%>%`
    + create a unique id column with `mutate()`


```{r process-df}
wine_df <- wine_ratings %>% 
  select(-X1) %>% 
  replace_na(list(country = "missing", province = "missing", taster_name = "missing")) %>% 
  extract(col = title, into = "year", regex = "(\\d\\d\\d\\d)", convert = TRUE, remove = FALSE) %>% 
  mutate(year = ifelse(year > 2025, NA, year),
         year = ifelse(year < 1970, NA, year)) %>% 
  filter(
    !is.na(price),
    !is.na(points),
    !is.na(year),
    year > 1999
  ) %>% 
  mutate(
    wine_id = row_number()
  )
  
```

Boom... data cleaning... pretty much done... almost


# Exploratory Data Analysis

One of `tidyverse`'s/`R`'s best feature is the `ggplot2` package, which stands for the grammar of graphics ...2(the prophet Hadley retire the first iteration). 

**Why is the grammar of graphics awesome?**
 
 + It allows us to quickly create production level graphs by continuously adding layers.
 + There are three mandatory things  *Data*, *Aesthetics*, & *Geometries* we need to define from `ggplot2` perspective, but we have control to or add the other layers when needed(*Facets*, *Statistics*, *Coordinates*, & `Theme`)

![](./imgs/grammar-of-graphics.png)

We will want to start making some plots to start understanding our data. One of the goals for this analysis is making a model(s) to predict the wine rating(`points`) of individual reviews(which is also why we cleaned the data).

Things to explore:

+ plot distribution of `points`
+ plot distribution of `price`
+ plot count of reviews over time
+ plot anything else you can think of


GGplot in action a code breakdown:

+ plot distribution of points
  + we take our data `wine_df` then `%>%`
  + use the `ggplot()` and 
  + give an aesthetic `aes()` and add `+` 
  + a geometry `geom_histogram()` and add `+`
  + (optional) a theme `theme_minimal()` and add `+`
  + (optional) a title `labs()`

```{r}
# check the distribution of points
wine_df %>%
    ggplot(aes(x = points)) + 
    geom_histogram( binwidth = 1) + 
    theme_minimal() + 
    labs(
      title = "Wine Ratings(Points) are normally Distributed",
      subtitle = "",
      caption = "*Having a normal distribution for our target variable is good for a linear model",
      y = "Count",
      x = "Wine Rating"
    )
```

+ plot distribution of `price`
  + Notice how the distribution is heavily skewed.  From a modeling standpoint we would want to scale that variable to see if it is *Log Normally Distributed*.
    + Why does that help? 
      + Certain types of models operated off of assumptions, in this case, the linear model we want to use demands that our dependent variable(`points`) needs to be normally distribute.  It also operates best when our independent variable(s)(`price`) are normally distributed as well.

  
```{r}
wine_df %>% 
  ggplot(aes(x = price)) + 
    geom_histogram(bins = 30)
```
  
  + If we add `+` `scale_x_log10()` to our plot, we see that our price variable is *Log Normally Distributed*.
  
```{r}
wine_df %>% 
  ggplot(aes(x = price)) + 
    geom_histogram(bins = 30) + 
    scale_x_log10()
```


+ plot count of reviews over time
  + here we can see the number of reviews in each year.

```{r}
wine_df %>% 
  group_by(year) %>% 
  summarise(count = n()) %>% 
  ggplot(aes(x = year, y = count))+ 
    geom_col()
```


# Model Building

We have looked at a few variables(even though you should explore more!), but we only have a loose understanding in a uni-variate or by-variate sense.  Modeling allows us to create a better understanding of our data while controlling for several variables.

+ Jordan's views on Modeling:
  + Start Simple: Linear Models such as Ordinary Least Squares regression are great as long as the assumptions are adhered to(see plotting explanations).
  + *"The most that can be expected from any model is that it can supply a useful approximation to reality: All models are wrong; some models are useful"* ~ George Box
    + Models generalize a problem they have variance, but knowing that let's us know how to utilize what they tell us.
  + There is No Free Lunch Theorem: States that without running models and comparing, no algorithm is said to work better than any other.
    + Basically states that there isn't one true model that works well on all problems.
    + You are not going to build a neural net on a data set of 10,000 observations


Modeling Objectives:

+ build a linear model
+ evaluate our model



Building a linear model a code breakdown:
+ build a linear model 

```{r}
# train model
lm_mod <- wine_df %>% 
  mutate(
    country = fct_lump(country, n = 10)
  ) %>% 
  lm(points ~ log2(price) + country + year, data = .) 
```

```{r model-check}
# check coeff
lm_mod %>% 
  tidy(conf.int = TRUE) %>% 
  filter(term != "(Intercept)") %>% 
  mutate(term = fct_reorder(term, estimate)) %>% 
  ggplot() + 
    geom_point(aes(x = term, y = estimate)) + 
    geom_errorbar(aes(x = term, ymin = conf.low, ymax = conf.high)) + 
    coord_flip()

# check model coefs contribution of variance explanation
lm_mod %>% 
  anova() %>% 
  tidy() %>% 
  mutate(
    contribution = sumsq/sum(sumsq)
  )
  
# check predictions
lm_mod %>% 
  augment() %>% 
  ggplot() + 
    geom_point(aes(y = points, x = .fitted), alpha = .1) + 
    geom_abline(color = "red")
```


# Text mining

+ tidy text data
+ most used words
+ which words are good
+ put into matrix form for modeling with glmnet
+ ...

```{r process-text}
# tidy text package
wine_words_df <- wine_df %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word %in% c("wine", "drink"),
         str_detect(word, "[a-z]"))

# check df 
wine_words_df
```

check top words

```{r}
wine_words_df %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = fct_reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip()
```

```{r filter-words}
wine_words_filtered_df <- wine_words_df %>%
  distinct(wine_id, word) %>%
  add_count(word) %>%
  filter(n >= 1000)
```


## which words are good?

 + enter the glmnet

```{r process-matrix}
# matrix package
# put into matrix
wine_word_matrix <- wine_words_filtered_df %>%
  cast_sparse(wine_id, word)

# 
wine_ids <- as.integer(rownames(wine_word_matrix))

# dependent variable
scores <- wine_df$points[wine_ids]

# add back price
wine_word_matrix_extra <- cbind(wine_word_matrix, log_price = log2(wine_words_df$price[wine_ids]))
```


Now let's run a penalized regression

```{r fit-glmnet}
# doparallel package
# glmnet package 

# set up parallel processing
registerDoParallel(4)

# create a cross validated model
glmnet_mod <- cv.glmnet(
    x = wine_word_matrix_extra
  , y = scores
  , family = c("gaussian")
  , parallel = TRUE
)
```

## check glmnet

```{r}
# you can see the impact of lambda on terms coefficients
glmnet_mod$glmnet.fit %>%
  tidy() %>%
  filter(term %in% c("rich", "black", "simple", "complex", "vineyard", "concentrated")) %>%
  ggplot(aes(lambda, estimate, color = term)) +
  geom_line() +
  scale_x_log10() +
  geom_hline(lty = 2, yintercept = 0) + 
  labs(
    title = "Lambda's impact on Coefficients"
  )

# smaller the penalty the more terms in the model
glmnet_mod$glmnet.fit %>%
  tidy() %>%
  count(lambda) %>%
  ggplot(aes(lambda, n)) +
  geom_line() +
  scale_x_log10() + 
  labs(
    title = "As Lambda Increases(Our Penalty) the Number of our Terms Decreases",
    y = "Number of Terms",
    x = "Lambda(Penalty)"
  )

# what's the best lambda
plot(glmnet_mod)
```



## Creating our own lexicon

```{r create-lexicon}
lexicon_df <- glmnet_mod$glmnet.fit %>%
  tidy() %>%
  filter(lambda == glmnet_mod$lambda.1se,
         term != "(Intercept)",
         term != "log_price") %>%
  select(word = term, coefficient = estimate)
```



```{r}
lexicon_df %>%
  arrange(coefficient) %>%
  group_by(direction = ifelse(coefficient < 0, "Negative", "Positive")) %>%
  top_n(16, abs(coefficient)) %>%
  ungroup() %>%
  mutate(word = fct_reorder(word, coefficient)) %>%
  ggplot(aes(word, coefficient, fill = direction)) +
  geom_col() +
  coord_flip() +
  labs(x = "",
       y = "Estimated effect of the word on the score",
       title = "What words are predictive of a wine's score?")
```


```{r}
wine_words_df %>%
  filter(wine_id %in% sample(unique(wine_id), 4)) %>%
  distinct(word, title, points) %>%
  mutate(wine = paste0(str_trunc(title, 40), " (", points, ")")) %>%
  inner_join(lexicon_df, by = "word") %>%
  mutate(word = fct_reorder(word, coefficient)) %>%
  ggplot(aes(word, coefficient, fill = coefficient > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~ wine, scales = "free_y") +
  labs(title = "How a lasso regression would predict each wine's score",
       subtitle = "Using a lasso regression with an extra term for price",
       x = "",
       y = "Effect on score")
```












